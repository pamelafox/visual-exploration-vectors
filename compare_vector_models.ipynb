{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing vector spaces\n",
    "\n",
    "Let's compare the vector embeddings from different models. We will use the following models:\n",
    "\n",
    "- Word2Vec, trained on the Google News dataset\n",
    "- OpenAI text-embedding-ada002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in vectors from openai and googlenews\n",
    "import json\n",
    "\n",
    "with open('vectors_word2vec-google-news.json') as f:\n",
    "    vectors_word2vec = json.load(f)\n",
    "    \n",
    "with open('vectors_openai_ada.json') as f:\n",
    "    vectors_ada = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def render_vector(vector):\n",
    "    \"\"\"Visualize the values of the vector in a bar chart\n",
    "\n",
    "    Args:\n",
    "    vector (list): a list of floating point values\n",
    "    \"\"\"\n",
    "    plt.bar(range(len(vector)), vector)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Vector')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_vector(vectors_word2vec['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_vector(vectors_ada['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_vector(vectors_ada['dog'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations with dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def perform_pca(vectors: dict):\n",
    "    \"\"\"Perform PCA on the word vectors and return the PCA-transformed vectors\"\"\"\n",
    "    X = np.array(list(vectors.values()))\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    X_pca = {list(vectors.keys())[i]: X_pca[i] for i in range(len(vectors))}\n",
    "    return X_pca\n",
    "\n",
    "\n",
    "vectors_google_pca = perform_pca(vectors_word2vec)\n",
    "vectors_openai_pca = perform_pca(vectors_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_vectors_3d(vectors: dict, title: str):\n",
    "    \"\"\"Render 3-dimensional vectors (key: [,,,]) in a 3D plot\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # Plot the vectors as points\n",
    "    for key, vector in vectors.items():\n",
    "        ax.scatter(vector[0], vector[1], vector[2])\n",
    "        ax.text(vector[0], vector[1], vector[2], key)\n",
    "\n",
    "    # Figure out the max and min values for each dimension\n",
    "    min_x = min(v[0] for v in vectors.values())\n",
    "    max_x = max(v[0] for v in vectors.values())\n",
    "    min_y = min(v[1] for v in vectors.values())\n",
    "    max_y = max(v[1] for v in vectors.values())\n",
    "    min_z = min(v[2] for v in vectors.values())\n",
    "    max_z = max(v[2] for v in vectors.values())\n",
    "    ax.set_xlim([min_x, max_x])\n",
    "    ax.set_ylim([min_y, max_y])\n",
    "    ax.set_zlim([min_z, max_z])\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get subset of the vectors for plotting\n",
    "words = ['queen', 'king', 'president', 'computer', 'dog', 'cat', 'car', 'boat', 'house', 'tree', 'god', 'technology']\n",
    "word_pca_vectors_google = {word: vectors_google_pca[word] for word in words}\n",
    "word_pca_vectors_openai = {word: vectors_openai_pca[word] for word in words}\n",
    "\n",
    "\n",
    "# Plot the vectors\n",
    "render_vectors_3d(word_pca_vectors_google, 'Word2Vec')\n",
    "render_vectors_3d(word_pca_vectors_openai, 'OpenAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Compute the cosine similarity between two vectors\"\"\"\n",
    "    dot_product = sum([a * b for a, b in zip(v1, v2)])\n",
    "    magnitude = (sum([a**2 for a in v1]) * sum([a**2 for a in v2])) ** 0.5\n",
    "    return dot_product / magnitude\n",
    "\n",
    "def most_similar(word: str, vectors: dict) -> list[list]:\n",
    "    \"\"\"Return the 10 most similar words and similarities to the given word\"\"\"\n",
    "    word_vector = vectors[word]\n",
    "    similarities = {w: cosine_similarity(word_vector, vector) for w, vector in vectors.items()}\n",
    "    most_similar_words = sorted(similarities, key=similarities.get, reverse=True)\n",
    "    return pd.DataFrame([(word, similarities[word]) for word in most_similar_words[:10]], columns=['word', 'similarity'])\n",
    "\n",
    "word = 'dog'\n",
    "most_similar(word, vectors_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(word, vectors_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_histogram(word: str, vectors: dict, model_name: str):\n",
    "    \"\"\"Plot a histogram of the cosine similarities of the word to all other words\"\"\"\n",
    "    word_vector = vectors[word]\n",
    "    similarities = [cosine_similarity(word_vector, vectors[w]) for w in vectors if w != word]\n",
    "    plt.hist(similarities, bins=20)\n",
    "    plt.xlabel('Cosine similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{model_name}: Similarity of {word} to all words')\n",
    "    plt.show()\n",
    "\n",
    "cosine_similarity_histogram('dog', vectors_word2vec, 'Word2Vec Google News')\n",
    "cosine_similarity_histogram('dog', vectors_ada, 'OpenAI ada-002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Why are Cosine Similarities of Text embeddings almost always positive?](https://vaibhavgarg1982.medium.com/why-are-cosine-similarities-of-text-embeddings-almost-always-positive-6bd31eaee4d5)\n",
    "* [Expected Angular Differences in Embedding Random Text?](https://community.openai.com/t/expected-angular-differences-in-embedding-random-text/28577)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
